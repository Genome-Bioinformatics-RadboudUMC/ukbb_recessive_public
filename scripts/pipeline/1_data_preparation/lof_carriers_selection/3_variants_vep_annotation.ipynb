{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a99f7a7",
   "metadata": {},
   "source": [
    "This notebook contains steps taken for the cohort data processing and variant selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de6dec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hail as hl\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import csv\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3d10561",
   "metadata": {},
   "source": [
    "Steps of processing the raw data from the RAP platform, all paths are relative to some workfolder: \n",
    "\n",
    "1. Data from RAP were copied into ---> `data_450k/vcfs/chrXX_lof.vcf.bgz`\n",
    "\n",
    "2. This VCFs were re-saved into matrix tables ---> `data_450k/matrix_tables/chrXX_lof.mt`\n",
    "\n",
    "3. Normalize genotypes into 0/1 format and represent variants in minimal form. Output 'chrom', 'pos', 'ref', 'alt', 'updated GT', 'locus', 'alleles', 'GT', 's' fields ---> `data_450k/annotations/chrXX_lof.normed.csv`\n",
    "\n",
    "4. Select unique variants, leave 'chrom', 'pos', 'ref', 'alt', 'cohort_cnt' fields  ---> `data_450k/annotations/chrXX_lof.normed.unique.csv`. This is done to speed up VEP annotation step.\n",
    "\n",
    "5. VEP annotation:\n",
    "\n",
    "    a. Transform data to VEP input format (VCF) ---> `data_450k/annotations/chrXX_lof.normed.unique.vcf`\n",
    "\n",
    "    b. Annotate VCF with VEP ---> `data_450k/annotations/chrXX_lof.vep.annotated.csv`\n",
    "\n",
    "6. Select LoF singletons ---> `data_450k/annotations/all_singetones_annotated.csv`\n",
    "\n",
    "7. Create files with individual's LoF singletons ---> `data_450k/sample_lofs/*`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9d60bb",
   "metadata": {},
   "source": [
    "# 1. Save VCFs as MatrixTables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c65f699",
   "metadata": {},
   "source": [
    "We run this code for each VCF file, saving them as MatrixTables to speed up calculations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b3d58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hail as hl\n",
    "import glob\n",
    "\n",
    "WORKFOLDER = '...' # should be defined\n",
    "\n",
    "# iterate over all vcf files \n",
    "for vcf_path in glob.glob(f\"{WORKFOLDER}/data_450k/vcfs/*.vcf.bgz\"):\n",
    "    \n",
    "    # read vcf file\n",
    "    vcf = hl.methods.import_vcf(vcf_path,\n",
    "                                force_bgz=True,\n",
    "                                reference_genome='GRCh38', \n",
    "                                array_elements_required=False,\n",
    "                                block_size=64)\n",
    "    print (f\"Processing {vcf_path}...\", flush=True)\n",
    "    # save vcf as matrix table\n",
    "    vcf.write(vcf_path.replace('vcfs', 'matrix_tables').replace('.vcf.bgz', '.mt'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31dee1cd",
   "metadata": {},
   "source": [
    "# 2. Normalize variant calls "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c980bd",
   "metadata": {},
   "source": [
    "Since variants came from multi-sample VCF file, they were not in their \"minimal\" form. Therefore, it was necessary to convert them to their minimal representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c249435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKFOLDER = '...' # should be defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e64c557",
   "metadata": {
    "code_folding": [
     2,
     26,
     33,
     78,
     106
    ]
   },
   "outputs": [],
   "source": [
    "# minimal representation of the variant\n",
    "\n",
    "def get_minimal_representation(pos, ref, alt): \n",
    "    \"\"\"\n",
    "    Creates minimal representation for alleles. \n",
    "    \n",
    "    Taken from here:\n",
    "        http://www.cureffi.org/2014/04/24/converting-genetic-variants-to-their-minimal-representation/\n",
    "    \"\"\"\n",
    "    # If it's a simple SNV, don't remap anything\n",
    "    if len(ref) == 1 and len(alt) == 1: \n",
    "        return pos, ref, alt\n",
    "    else:\n",
    "        # strip off identical suffixes (from the end)\n",
    "        while(alt[-1] == ref[-1] and min(len(alt),len(ref)) > 1):\n",
    "            alt = alt[:-1]\n",
    "            ref = ref[:-1]\n",
    "            \n",
    "        # strip off identical prefixes (from the start) and increment position\n",
    "        while(alt[0] == ref[0] and min(len(alt),len(ref)) > 1):\n",
    "            alt = alt[1:]\n",
    "            ref = ref[1:]\n",
    "            pos += 1\n",
    "            \n",
    "        return pos, ref, alt \n",
    "    \n",
    "def get_new_genotype(old_genotype, allele):\n",
    "    \"\"\"\n",
    "        Converts genotype from Hail's multi-allelic vcf representation \n",
    "        to biallelic 0/1, 1/0, 1/1\n",
    "    \"\"\"\n",
    "    result_genotype = np.array([0, 0])\n",
    "    result_genotype[np.array(old_genotype) == allele] = 1\n",
    "    \n",
    "    return \"/\".join(map(str, result_genotype.tolist()))\n",
    "\n",
    "# alleles parsing (includes application of the minimal representation)\n",
    "def parse_alleles_from_hail(row):\n",
    "    \"\"\"\n",
    "        Splits multi-allelic variants (e.g. 1/2) into biallelic 1/0, 0/1, 1/1\n",
    "        Gets minimal representation of the variants\n",
    "    \"\"\"\n",
    "    \n",
    "    # cast types \n",
    "    contig, position = tuple(row['locus'].split(':'))\n",
    "    position = int(position)\n",
    "    alleles = eval(row['alleles']) # should be list of alleles at locus: ['A', 'C', 'AT']\n",
    "\n",
    "    genotype = [int(x) for x in row['GT'].split('/')]# should be [0, 1] for 0/1 etc/\n",
    "    \n",
    "    result_rows = []\n",
    "    \n",
    "    # iterates over genotypes, returns more that one row for heterozygous non ref (1/2)\n",
    "    for gt in set(genotype):\n",
    "        \n",
    "        # skip reference\n",
    "        if gt == 0:\n",
    "            continue\n",
    "        \n",
    "        result_row = row.copy()\n",
    "        \n",
    "        # parse alleles  \n",
    "        result_row['ref'] = alleles[0] # picks reference allele\n",
    "        result_row['alt'] = alleles[gt] # picks alternative allele\n",
    "        \n",
    "        # parse genotype\n",
    "        result_row['updated GT'] = get_new_genotype(genotype, gt) # returns 0/1 for [0, 2], [1, 2]\n",
    "\n",
    "        # get minimal representation\n",
    "        result_row['pos'], result_row['ref'], result_row['alt'] = get_minimal_representation(\n",
    "            position,\n",
    "            result_row['ref'], \n",
    "            result_row['alt']) \n",
    "        \n",
    "        result_row['chrom'] = contig\n",
    "                \n",
    "        result_rows.append(result_row)\n",
    "        \n",
    "    return result_rows\n",
    "\n",
    "# processing of a file\n",
    "def process_and_save_vcf(input_path, output_path=None):\n",
    "    \"\"\"\n",
    "        Applies all genotype and variant formats transformation to every row of the input file \n",
    "        and saves the result as gzipped csv\n",
    "    \"\"\"\n",
    "    \n",
    "    if output_path is None:\n",
    "        output_path = input_path.replace('.csv', '.norm.csv')\n",
    "\n",
    "    with open(input_path, 'r') as in_csvfile, open(output_path, 'w') as out_csvfile:\n",
    "        \n",
    "        # define output csv file field names\n",
    "        fieldnames = ['chrom', 'pos', 'ref', 'alt', 'updated GT', 'locus', 'alleles', 'GT', 's']\n",
    "        \n",
    "        # create writer\n",
    "        processed_data = csv.DictWriter(out_csvfile, fieldnames=fieldnames)\n",
    "        processed_data.writeheader()\n",
    "        \n",
    "        # create reader\n",
    "        data = csv.DictReader(in_csvfile, delimiter='\\t')\n",
    "        parsed_data = []\n",
    "        \n",
    "        # process every line of the input file\n",
    "        for row in data:\n",
    "            for processed_row in parse_alleles_from_hail(row):\n",
    "                processed_data.writerow(processed_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8668fedd",
   "metadata": {},
   "source": [
    "First, we extract all genotypes from the cohort, that are not homozygous reference and save as .csv files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3ddcb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save data as .csv\n",
    "\n",
    "for filename in glob.glob(f\"{WORKFOLDER}/data_450k/matrix_tables/*.mt\"):\n",
    "    # output filename\n",
    "    out_filename = filename.replace('matrix_tables', 'annotations').replace('.mt', '.raw.csv')\n",
    "    \n",
    "    print ('Processing', filename)\n",
    "    \n",
    "    # read matrix table\n",
    "    data = hl.read_matrix_table(filename)\n",
    "    \n",
    "    # write every genotype as separate row\n",
    "    data.filter_entries(~data.GT.is_hom_ref()).entries().GT.export(out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f317d874",
   "metadata": {},
   "source": [
    "Normalize variants representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63038ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for filename in glob.glob(f\"{WORKFOLDER}/data_450k/annotations/*.raw.csv\"):\n",
    "    print ('Processing', filename, flush=True)\n",
    "\n",
    "    out_filename = filename.replace('.raw.csv', '.normed.csv')\n",
    "\n",
    "    process_and_save_vcf(input_path=filename, output_path=out_filename)\n",
    "    \n",
    "    print ('Writing to', out_filename, flush=True)\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee81429",
   "metadata": {},
   "source": [
    "# 3. Select unique variants with cohort counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c2bcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in glob.glob(f\"{WORKFOLDER}/data_450k/annotations/*.normed.csv\"):\n",
    "\n",
    "  print ('Processing', filename, flush=True)\n",
    "\n",
    "  out_filename = filename.replace('.normed.csv', '.normed.unique.csv')\n",
    "  \n",
    "  # get unique variants\n",
    "  df = (\n",
    "    pd.read_csv(filename)\n",
    "    .groupby(['chrom', 'pos', 'ref', 'alt'])\n",
    "    .agg({'s': 'count'})\n",
    "    .rename(columns={'s': 'cohort_cnt'})\n",
    "    .reset_index()\n",
    "  )\n",
    "\n",
    "  df.to_csv(out_filename, index=False)\n",
    "\n",
    "  print ('Writing to', out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6abe01f",
   "metadata": {},
   "source": [
    "# 4. VEP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ef2fc2",
   "metadata": {},
   "source": [
    "## a. Converting to VCF header format, necessary for subsequent VEP annotation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8a1b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create VCF format from data frame\n",
    "def make_vcf_format(df2):\n",
    "    df2 = df2.rename(columns={'chrom':'#CHROM', 'pos':'POS', 'ref': 'REF', 'alt':'ALT'})\n",
    "\n",
    "    df2['ID'] = '.'\n",
    "    df2['QUAL'] = '.'\n",
    "    df2['FILTER'] = '.'\n",
    "    df2['INFO'] = '.'\n",
    "\n",
    "    df2 = df2[['#CHROM', 'POS', 'ID', 'REF', 'ALT', 'QUAL', 'FILTER', 'INFO']]\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0756aa90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for filename in glob.glob(f\"{WORKFOLDER}/data_450k/annotations/*.normed.unique.csv\"):\n",
    "    print ('Processing', filename, flush=True)\n",
    "\n",
    "    out_filename = filename.replace('.normed.unique.csv', '.normed.unique.vcf')\n",
    "    \n",
    "    df = pd.read_csv(filename)\n",
    "    df = make_vcf_format(df)\n",
    "    df.to_csv(out_filename, sep='\\t', index=False)\n",
    "    \n",
    "    print ('Writing to', out_filename, flush=True)\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6363d0",
   "metadata": {},
   "source": [
    "## b. Create and run VEP annotation commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d29e443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vep(in_file, annotated_file):\n",
    "    job = f\"\"\"\n",
    "vep --no_stats \\\n",
    "\t--fasta ... \\\n",
    "    --format vcf \\\n",
    "    --tab \\\n",
    "\t-i {in_file} \\\n",
    "\t-o {annotated_file} \\\n",
    "\t--gff ... \\\n",
    "\t--assembly GRCh38 \\\n",
    "\t--symbol --numbers --hgvs --hgvsg \\\n",
    "\t--plugin LoF,loftee_path:...,check_complete_cds:1,max_scan_distance:30 \\\n",
    "\t--plugin REVEL,...\\\n",
    "\t--dir_plugins ... --fork 6\n",
    "\"\"\"\n",
    "    return job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a940faa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for filename in glob.glob(f\"{WORKFOLDER}/data_450k/annotations/*.normed.unique.vcf\"):\n",
    "    out_filename = filename.replace('.normed.unique.vcf', '.vep.annotated.csv')\n",
    "    \n",
    "    jobs_filename = (\n",
    "        filename\n",
    "        .replace('data_450k/annotations/', 'jobs_450k/2_vep_')\n",
    "        .replace('.normed.unique.vcf', '.job')\n",
    "    )\n",
    "        \n",
    "    vep_command = create_vep(in_file=filename, annotated_file=out_filename)\n",
    "        \n",
    "    with open(jobs_filename, mode='w') as f:\n",
    "        f.writelines(vep_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92324259",
   "metadata": {},
   "source": [
    "These jobs should be executed for every chromosome. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b85a8c8",
   "metadata": {},
   "source": [
    "# 5. Variant selection: LoF singletons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01beb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_lof(consequence_terms):\n",
    "    \"\"\"\n",
    "        Returns True if consequence terms contains loss-of-function variant.\n",
    "    \"\"\"\n",
    "    is_lof = (\n",
    "        consequence_terms.str.contains(\"splice_acceptor_variant\") | \n",
    "        consequence_terms.str.contains(\"splice_donor_variant\") | \n",
    "        consequence_terms.str.contains(\"stop_gained\") | \n",
    "        consequence_terms.str.contains(\"frameshift_variant\")\n",
    "    )\n",
    "    \n",
    "    return is_lof\n",
    "\n",
    "\n",
    "def get_variant_class(row):\n",
    "    \"\"\"\n",
    "    Defining variant type (SNP, insertion, deletion)\n",
    "    based on reference and alternative alleles\n",
    "    Parameters:\n",
    "    row: DataFrame row\n",
    "    Returns:\n",
    "    Substitution, Deletion, Insertion or Unknown\n",
    "    \"\"\"\n",
    "    reference, variant = row['ref'], row['alt']\n",
    "\n",
    "    if len(reference) == len(variant):\n",
    "        return 'Substitution'\n",
    "    elif len(reference) > len(variant):\n",
    "        return 'Deletion'\n",
    "    elif len(reference) < len(variant):\n",
    "        return 'Insertion'\n",
    "\n",
    "    return 'Unknown'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408bb6e2",
   "metadata": {},
   "source": [
    "Get original variants from RAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356ac8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all variants found in the cohort\n",
    "original_variants = []\n",
    "\n",
    "for filename in glob.glob(f\"{WORKFOLDER}/data_450k/annotations/*.normed.unique.csv\"):\n",
    "    original_variants.append(pd.read_csv(filename))\n",
    "    \n",
    "original_variants = pd.concat(original_variants)\n",
    "\n",
    "# annotate with variant type\n",
    "original_variants['variant_type'] = original_variants.apply(get_variant_class, axis=1)\n",
    "\n",
    "print (\"Total normed unique variants:\", original_variants.shape)\n",
    "\n",
    "original_variants.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf422aa",
   "metadata": {},
   "source": [
    "Update InDels representation to match VEP: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f188ffa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Keep representation the same for substitutions\n",
    "indels = original_variants['variant_type']!='Substitution'\n",
    "\n",
    "original_variants['new pos'] = original_variants['pos'].copy()\n",
    "original_variants['new ref'] = original_variants['ref'].copy()\n",
    "original_variants['new alt'] = original_variants['alt'].copy()\n",
    "\n",
    "# update representation for InDels by ignoring first matched nucleotide, i.e. AT>A --> T>\n",
    "original_variants.loc[indels, 'new pos'] = original_variants.loc[indels, 'new pos'] + 1\n",
    "original_variants.loc[indels, 'new ref'] = original_variants.loc[indels, 'new ref'].apply(lambda x: x[1:])\n",
    "original_variants.loc[indels, 'new alt'] = original_variants.loc[indels, 'new alt'].apply(lambda x: x[1:])\n",
    "\n",
    "# Create Key for matching with VEP annotations and delete redundant information\n",
    "original_variants['key'] = (\n",
    "    original_variants['chrom'] + '_' + \n",
    "    original_variants['new pos'].astype(str) + '_' + \n",
    "    original_variants['new ref'] + '_' + \n",
    "    original_variants['new alt']\n",
    ")\n",
    "\n",
    "original_variants = original_variants.drop(['new pos', 'new ref', 'new alt'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f0ddbf",
   "metadata": {},
   "source": [
    "Read VEP annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4bcacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read VEP annotations\n",
    "annotations = []\n",
    "\n",
    "for filename in glob.glob(f\"{WORKFOLDER}/data_450k/annotations/*.vep.annotated.csv\"):\n",
    "    df = pd.read_csv(filename, sep='\\t', skiprows=41).rename(\n",
    "        columns={'#Uploaded_variation': 'Uploaded_variation'})\n",
    "    annotations.append(df)\n",
    "    \n",
    "annotations = pd.concat(annotations)\n",
    "\n",
    "print (\"Total annotated variants, all transcripts:\", annotations.shape)\n",
    "print (\"Total annotated unique variants:\", annotations['Uploaded_variation'].drop_duplicates().shape)\n",
    "\n",
    "\n",
    "annotations.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb6e983",
   "metadata": {},
   "source": [
    "Select high-quality LoF variants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeb9a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select LoF\n",
    "annotations = annotations[is_lof(annotations['Consequence'])]\n",
    "# select high-quality \n",
    "annotations = annotations[annotations['LoF'] == 'HC']\n",
    "\n",
    "\n",
    "print (\"Total high quality LoF variants, all transcripts:\", annotations.shape)\n",
    "print (\"Total high quality LoF  unique variants:\", annotations['Uploaded_variation'].drop_duplicates().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd92c0f",
   "metadata": {},
   "source": [
    "Creating the key for merging with cohort variants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a713be68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse variant from VEP output\n",
    "columns = annotations.columns.tolist()\n",
    "\n",
    "annotations['chrom'] = annotations['Uploaded_variation'].apply(lambda x: x.split('_')[0])\n",
    "annotations['pos'] = annotations['Uploaded_variation'].apply(lambda x: int(x.split('_')[1]))\n",
    "annotations['ref'] = annotations['Uploaded_variation'].apply(lambda x: x.split('_')[2].split('/')[0])\n",
    "annotations['alt'] = annotations['Uploaded_variation'].apply(lambda x: x.split('_')[2].split('/')[1])\n",
    "\n",
    "annotations.loc[annotations['ref']=='-', 'ref'] = ''\n",
    "annotations.loc[annotations['alt']=='-', 'alt'] = ''\n",
    "\n",
    "# remove redundant information\n",
    "annotations = annotations[['chrom', 'pos', 'ref', 'alt'] + ['SYMBOL']].drop_duplicates()\n",
    "\n",
    "print (\"Total high quality LoF variants, all genes:\", annotations.shape)\n",
    "print (\"Total high quality LoF unique variants\", annotations[['chrom', 'pos', 'ref', 'alt']].drop_duplicates().shape)\n",
    "\n",
    "# add information about variant type\n",
    "annotations['variant_type'] = annotations.apply(get_variant_class, axis=1)\n",
    "\n",
    "# create key\n",
    "annotations['key'] = (\n",
    "    annotations['chrom'] + '_' + \n",
    "    annotations['pos'].astype(str) + '_' + \n",
    "    annotations['ref'] + '_' + \n",
    "    annotations['alt']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f66f95",
   "metadata": {},
   "source": [
    "Filter original variants to contain high-quality LoF variants; \n",
    "\n",
    "Create new key for merged data based on original variant representation (for merging with individual's variant data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06f36b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge high-quality LoF with original data\n",
    "merged = annotations[['SYMBOL', 'key']].merge(original_variants, on='key')\n",
    "\n",
    "# generate new key\n",
    "merged['key'] = (\n",
    "    merged['chrom'] + '_' + \n",
    "    merged['pos'].astype(str) + '_' + \n",
    "    merged['ref'] + '_' + \n",
    "    merged['alt']\n",
    ")\n",
    "\n",
    "print (\"Total high quality LoF variants all genes merged with cohort\", merged.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546cf20d",
   "metadata": {},
   "source": [
    "Leave only singletons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33605a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "singletones = merged[merged['cohort_cnt'] == 1]\n",
    "\n",
    "print (\"Number of singletones all genes:\", singletones.shape[0])\n",
    "print (\"Number of unique singletones:\", singletones[['chrom', 'pos', 'ref', 'alt']].drop_duplicates().shape)\n",
    "\n",
    "\n",
    "singletones.to_csv(f\"{WORKFOLDER}/data_450k/annotations/all_singetones_annotated.csv\", sep='\\t', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "674ac938",
   "metadata": {},
   "source": [
    "# 6.Filter individual files for singletones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b352ba2",
   "metadata": {},
   "source": [
    "Filter individual's files so, that they would only contain singleton LoFs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53029d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ukbb_recessive.data_collection.variants import VariantFeaturesLoF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2290a735",
   "metadata": {},
   "outputs": [],
   "source": [
    "rap_files = glob.glob(f\"{WORKFOLDER}/data_450k/annotations/*.normed.csv\")\n",
    "\n",
    "print (len(rap_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f510c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_features = VariantFeaturesLoF()\n",
    "\n",
    "variant_features.filter_lofs_in_samples(\n",
    "    rap_files=rap_files, \n",
    "    output_folder=f\"{WORKFOLDER}/data_450k/sample_lofs\", \n",
    "    all_lof_file=f\"{WORKFOLDER}/data_450k/annotations/all_singetones_annotated.csv\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (variant_prio)",
   "language": "python",
   "name": "variant_prio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
