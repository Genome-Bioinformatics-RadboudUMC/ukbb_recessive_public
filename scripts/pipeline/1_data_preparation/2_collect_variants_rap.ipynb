{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains a pipeline for the variant selection process, that is executed on RAP and is ainmed to reduce the amount of variants, downloaded from RAP. We focus on the selection of the variants, that are located in our 1929 recessive genes and covered in >90% of the cases with at least 15x. \n",
    "\n",
    "\n",
    "\n",
    "This notebook should be placed in UKBB Research Analysis Platform.\n",
    "\n",
    "The cell that contains chromosome variable should be tagged as explained [here](https://papermill.readthedocs.io/en/latest/usage-parameterize.html#designate-parameters-for-a-cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to running the notebook, the following data should be uploaded to the RAP:\n",
    "\n",
    " - The GRCh38 coordinates of the targeted regions `xgen_plus_spikein.GRCh38.bed` from https://biobank.ndph.ox.ac.uk/ukb/refer.cgi?id=3803 .\n",
    "\n",
    " - The GRCh38 coordinates of the 1929 recessive genes `transcripts_exons_hg38_merged_10bp.bed`\n",
    "\n",
    " - List of related samples that needs to be removed `related_samples_to_remove_final.txt` (generated on the previous step).\n",
    "\n",
    " - In this section we also download `pvcf_blocks.txt` as the data for each chromosome is splitted across several files, marked by block_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "sc = pyspark.SparkContext()\n",
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pip-installed Hail requires additional configuration options in Spark referring\n",
      "  to the path to the Hail Python module directory HAIL_DIR,\n",
      "  e.g. /path/to/python/site-packages/hail:\n",
      "    spark.jars=HAIL_DIR/hail-all-spark.jar\n",
      "    spark.driver.extraClassPath=HAIL_DIR/hail-all-spark.jar\n",
      "    spark.executor.extraClassPath=./hail-all-spark.jarRunning on Apache Spark version 2.4.4\n",
      "SparkUI available at http://ip-10-60-94-211.eu-west-2.compute.internal:8081\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.78-b17627756568\n",
      "LOGGING: writing to /opt/notebooks/hail-20221118-1438-0.2.78-b17627756568.log\n"
     ]
    }
   ],
   "source": [
    "import hail as hl\n",
    "hl.init(sc=sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# this cell is tagged parameters (will act as a command line argument)\n",
    "chromosome = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 14:39:03 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'f0' as type str (user-supplied)\n",
      "  Loading field 'f1' as type int32 (user-supplied)\n",
      "  Loading field 'f2' as type int32 (user-supplied)\n",
      "2022-11-18 14:39:04 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'f0' as type str (user-supplied)\n",
      "  Loading field 'f1' as type int32 (user-supplied)\n",
      "  Loading field 'f2' as type int32 (user-supplied)\n"
     ]
    }
   ],
   "source": [
    "# load bed for 1929 genes\n",
    "bed_path = 'file:///mnt/project/Uploaded_data/transcripts_exons_hg38_merged_10bp.bed'\n",
    "\n",
    "recode = {f\"{i}\":f\"chr{i}\" for i in (list(range(1, 23)) + ['X', 'Y'])}\n",
    "\n",
    "bed = hl.import_bed(bed_path, reference_genome='GRCh38', contig_recoding=recode)\n",
    "\n",
    "# load bed for target sequencing region\n",
    "target_bed_path = 'file:///mnt/project/Uploaded_data/xgen_plus_spikein.GRCh38.bed'\n",
    "\n",
    "bed_target = hl.import_bed(target_bed_path, reference_genome='GRCh38')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "# read pvcf blocks file\n",
    "pvcf_blocks = pd.read_csv(\"https://biobank.ctsu.ox.ac.uk/crystal/ukb/auxdata/pvcf_blocks.txt\", \n",
    "                          sep='\\t', header=None)\n",
    "pvcf_blocks.columns = ['row_id', 'chrom', 'block_id', 'start', 'end']\n",
    "\n",
    "chromosome_pvcf_blocks = pvcf_blocks[pvcf_blocks['chrom'].astype(int) == int(chromosome)]['block_id'].tolist()\n",
    "\n",
    "print (len(chromosome_pvcf_blocks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and filter data by bed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we:\n",
    "\n",
    "1. Load all gvcfs associated with a chromosome.\n",
    "\n",
    "2. Leave only target regions and 1,929 recessive genes of interest described [here](https://hail.is/docs/0.2/guides/genetics.html#from-a-ucsc-bed-file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def load_filter_gvcf(gvcf_path):\n",
    "    \"\"\"\n",
    "        Reads pVCF file defined in `gvcf_path` \n",
    "        and leaves only target regions from `bed_target` and \n",
    "        1929 recessive genes from `bed`.\n",
    "    \"\"\"\n",
    "    print (f\"load {gvcf_path}\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    gvcf = hl.methods.import_vcf(gvcf_path, force_bgz=True, reference_genome='GRCh38', block_size=32)\n",
    "    gvcf = gvcf.filter_rows(hl.is_defined(bed[gvcf.locus]))\n",
    "    gvcf = gvcf.filter_rows(hl.is_defined(bed_target[gvcf.locus]))\n",
    "    \n",
    "    return gvcf\n",
    "\n",
    "def load_concatenate_gvcfs(gvcf_paths):\n",
    "    \"\"\"\n",
    "        Reads all gvcfs defined in `gvcf_paths` list\n",
    "    \"\"\"\n",
    "    gvcfs = [load_filter_gvcf(gvcf_path) for gvcf_path in gvcf_paths]\n",
    "    \n",
    "    return gvcfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate all gvcf paths for a given chromosome\n",
    "gvcf_paths = [\n",
    "    f'file:///mnt/project/Bulk/Exome sequences/'\n",
    "    f'Population level exome OQFE variants, pVCF format - final release/ukbXXXXX_c{chromosome}_b{idx}_v1.vcf.gz' for idx in chromosome_pvcf_blocks\n",
    "]\n",
    "\n",
    "# load all gvcfs for a given chromosome\n",
    "vcf = load_concatenate_gvcfs(gvcf_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create & load a list of unrelated samples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code should be executed once, when the list of the related samples arrives. Otherwise `CREATE_SAMPLES_LIST` should be `False`.\n",
    "\n",
    "It creates `unrelated_samples.txt` that is all samples minus `related_samples_to_remove_final.txt`. This file is used for LoF variant collection pipeline as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE_SAMPLES_LIST = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_SAMPLES_LIST:\n",
    "    # load samples that are related\n",
    "    with open('/mnt/project/Uploaded_data/related_samples_to_remove_final.txt', 'r') as f:\n",
    "        related_samples_list = f.readlines()\n",
    "\n",
    "    related_samples_list = [sample_id.strip() for sample_id in related_samples_list]\n",
    "\n",
    "    print (\"Related number of samples:\", len(related_samples_list))\n",
    "    \n",
    "    # get the list of all samples\n",
    "    samples = vcf[0].s.collect()\n",
    "\n",
    "    print (\"Original number of samples:\", len(samples))\n",
    "\n",
    "    # filter withdrawned\n",
    "    samples = [sample for sample in samples if not sample.startswith('W')]\n",
    "\n",
    "    print (\"Filtered withdrawn number of samples:\", len(samples))\n",
    "\n",
    "    # filter related\n",
    "    samples = [sample for sample in samples if not (sample in related_samples_list)]\n",
    "\n",
    "    print (\"Filtered related number of samples:\", len(samples))\n",
    "\n",
    "    # save\n",
    "    with open('unrelated_samples.txt', 'w') as f:\n",
    "        f.writelines([sample + '\\n' for sample in samples])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first time, you should now download `unrelated_samples.txt` and manually upload it to `Uploaded_data` folder.\n",
    "\n",
    "Later, there is no need to do that, you can proceed to the next piece of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1322654', '2611975', '2738629']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load samples that should be used in analysis and doesnt contain related samples\n",
    "\n",
    "with open('/mnt/project/Uploaded_data/unrelated_samples.txt', 'r') as f:\n",
    "    unrelated_samples = f.readlines()\n",
    "    \n",
    "unrelated_samples = [sample_id.strip() for sample_id in unrelated_samples]\n",
    "\n",
    "print (len(unrelated_samples))\n",
    "\n",
    "unrelated_samples[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code to select all variants of interest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter related & withdrawned samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we keep only unrelated samples, that were not withdrawn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leave only variants that are unrelated\n",
    "# and remove withdrawned sampples\n",
    "\n",
    "for idx in range(len(vcf)):\n",
    "    vcf[idx] = vcf[idx].filter_cols(hl.array(unrelated_samples).contains(vcf[idx].s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter by coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove all sites, that didn't pass our quality control (covered at least 15x in >=90% of the cohort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many samples has sufficient coverage >= 15\n",
    "vcf_annotated = []\n",
    "\n",
    "for idx in range(len(vcf)):\n",
    "    vcf_annotated.append(\n",
    "        vcf[idx].annotate_rows(variant_dp = hl.agg.sum(vcf[idx].DP >= 15))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcf_filtered = []\n",
    "\n",
    "for idx in range(len(vcf_annotated)):\n",
    "    # leave only locations with sufficient coverage in most locations\n",
    "    vcf_filtered_item = vcf_annotated[idx].filter_rows(vcf_annotated[idx].variant_dp >= len(unrelated_samples)*0.9)\n",
    "\n",
    "    # drop unused unfo\n",
    "    vcf_filtered_item = vcf_filtered_item.select_globals().select_rows().select_entries('GT')\n",
    "\n",
    "    vcf_filtered.append(vcf_filtered_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave only het_ref and hom_var "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove all homozygous reference genotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(vcf_filtered)):\n",
    "    \n",
    "    # leave only variants that contain non-ref allele\n",
    "    vcf_filtered[idx] = vcf_filtered[idx].filter_entries(~vcf_filtered[idx].GT.is_hom_ref())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate calculations and save data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we convert variants + genotype information into pandas table and save it for a future download. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 1 of 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 14:45:24 Hail: WARN: entries(): Resulting entries table is sorted by '(row_key, col_key)'.\n",
      "    To preserve row-major matrix table order, first unkey columns with 'key_cols_by()'\n",
      "2022-11-18 14:47:51 Hail: INFO: Coerced sorted dataset\n",
      "2022-11-18 14:47:55 Hail: INFO: Coerced sorted dataset\n",
      "2022-11-18 14:48:04 Hail: INFO: Coerced sorted dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 486.386498s\n",
      "\n",
      "Processing: 2 of 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 14:56:12 Hail: INFO: Coerced sorted dataset\n",
      "2022-11-18 14:56:15 Hail: INFO: Coerced sorted dataset\n",
      "2022-11-18 14:56:22 Hail: INFO: Coerced sorted dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 482.970929s\n",
      "\n",
      "Processing: 3 of 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 15:04:16 Hail: INFO: Coerced sorted dataset\n",
      "2022-11-18 15:04:16 Hail: INFO: Coerced sorted dataset\n",
      "2022-11-18 15:04:18 Hail: INFO: Coerced sorted dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 536.350031s\n",
      "\n",
      "Processing: 4 of 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 15:13:45 Hail: INFO: Coerced sorted dataset\n",
      "2022-11-18 15:13:46 Hail: INFO: Coerced sorted dataset\n",
      "2022-11-18 15:13:53 Hail: INFO: Coerced sorted dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 604.795341s\n",
      "\n",
      "Processing: 5 of 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 15:24:02 Hail: INFO: Coerced sorted dataset\n",
      "2022-11-18 15:24:03 Hail: INFO: Coerced sorted dataset\n",
      "2022-11-18 15:24:04 Hail: INFO: Coerced sorted dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 548.042084s\n",
      "\n",
      "Processing: 6 of 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 15:32:36 Hail: INFO: Coerced sorted dataset\n",
      "2022-11-18 15:32:37 Hail: INFO: Coerced sorted dataset\n",
      "2022-11-18 15:32:45 Hail: INFO: Coerced sorted dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 511.980043s\n",
      "\n",
      "Processing: 7 of 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 15:41:11 Hail: INFO: Coerced sorted dataset\n",
      "2022-11-18 15:41:11 Hail: INFO: Coerced sorted dataset\n",
      "2022-11-18 15:41:13 Hail: INFO: Coerced sorted dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 530.365661s\n",
      "\n",
      "Processing: 8 of 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 15:49:44 Hail: INFO: Coerced sorted dataset\n",
      "2022-11-18 15:49:44 Hail: INFO: Coerced sorted dataset\n",
      "2022-11-18 15:49:46 Hail: INFO: Coerced sorted dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 452.416757s\n",
      "\n",
      "Processing: 9 of 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 15:57:47 Hail: INFO: Coerced sorted dataset\n",
      "2022-11-18 15:57:48 Hail: INFO: Coerced sorted dataset\n",
      "2022-11-18 15:57:49 Hail: INFO: Coerced sorted dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 644.291446s\n",
      "\n",
      "Processing: 10 of 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 16:08:27 Hail: INFO: Coerced sorted dataset\n",
      "2022-11-18 16:08:28 Hail: INFO: Coerced sorted dataset\n",
      "2022-11-18 16:08:29 Hail: INFO: Coerced sorted dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 595.420629s\n",
      "\n",
      "Processing: 11 of 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 16:17:10 Hail: INFO: Coerced sorted dataset\n",
      "2022-11-18 16:17:12 Hail: INFO: Coerced sorted dataset\n",
      "2022-11-18 16:17:14 Hail: INFO: Coerced sorted dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 396.263435s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import gc\n",
    "\n",
    "for idx in range(len(vcf_filtered)):\n",
    "    print (f'Processing: {idx+1} of {len(vcf_filtered)}')\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    start = datetime.datetime.now()\n",
    "    \n",
    "    # flatten table \n",
    "    gt_entries = vcf_filtered[idx].entries()\n",
    "    \n",
    "    # convert to pandas\n",
    "    df = gt_entries.to_pandas()\n",
    "    df.to_csv(f'output.chr{chromosome}.part{idx}.csv.gz', compression='gzip')\n",
    "    \n",
    "    del df\n",
    "    gc.collect()\n",
    "    \n",
    "    # calculate duration\n",
    "    delta = datetime.datetime.now() - start\n",
    "    print (f'Elapsed time: {delta.total_seconds()}s')\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /opt/notebooks/hail*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following commands were used to start the code:\n",
    "\n",
    "```\n",
    "dx cd ..\n",
    "\n",
    "\n",
    "dx mkdir chr19\n",
    "dx cd chr19\n",
    "\n",
    "my_cmd=\"papermill 2_collect_variants_rap.ipynb 2_collect_variants_rap_chr19_output.ipynb -p chromosome 19\"\n",
    "\n",
    "dx run dxjupyterlab_spark_cluster --instance-type=mem2_ssd1_v2_x16 --instance-count=5 --priority=low --name=\"Run analysis chr19\" -icmd=\"$my_cmd\" -iduration=2160 -iin=\"../../2_collect_variants_rap.ipynb\" -ifeature=\"HAIL-0.2.78-VEP-1.0.3\"\n",
    "\n",
    "dx download chr1 -r\n",
    "dx download chr2 -r\n",
    "dx download chr3 -r\n",
    "dx download chr4 -r\n",
    "dx download chr5 -r\n",
    "dx download chr6 -r\n",
    "dx download chr7 -r\n",
    "dx download chr8 -r\n",
    "dx download chr9 -r\n",
    "dx download chr10 -r\n",
    "dx download chr11 -r\n",
    "dx download chr12 -r\n",
    "dx download chr13 -r\n",
    "dx download chr14 -r\n",
    "dx download chr15 -r\n",
    "dx download chr16 -r\n",
    "dx download chr17 -r\n",
    "dx download chr19 -r\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
